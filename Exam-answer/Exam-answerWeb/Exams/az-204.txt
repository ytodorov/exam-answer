###
You develop a software as a service (SaaS) offering to manage photographs. Users upload photos to a web service which then stores the photos in Azure
Storage Blob storage. The storage account type is General-purpose V2.
When photos are uploaded, they must be processed to produce and save a mobile-friendly version of the image. The process to produce a mobile-friendly version of the image must start in less than one minute.
You need to design the process that starts the photo processing.
Solution: Convert the Azure Storage account to a BlockBlobStorage storage account.
Does the solution meet the goal?
---
A. Yes
B. No *
---
Not necessary to convert the account, instead move photo processing to an Azure Function triggered from the blob upload..
Azure Storage events allow applications to react to events. Common Blob storage event scenarios include image or video processing, search indexing, or any file- oriented workflow.
Note: Only storage accounts of kind StorageV2 (general purpose v2) and BlobStorage support event integration. Storage (general purpose v1) does not support integration with Event Grid.

Azure Storage events allow applications to react to events, such as the creation and deletion of blobs. It does so without the need for complicated code or expensive and inefficient polling services. The best part is you only pay for what you use.

Blob storage events are pushed using Azure Event Grid to subscribers such as Azure Functions, Azure Logic Apps, or even to your own http listener. Event Grid provides reliable event delivery to your applications through rich retry policies and dead-lettering.
---
Reacting to Blob storage events;https://docs.microsoft.com/en-us/azure/storage/blobs/storage-blob-event-overview
###
You develop a software as a service (SaaS) offering to manage photographs. Users upload photos to a web service which then stores the photos in Azure
Storage Blob storage. The storage account type is General-purpose V2.
When photos are uploaded, they must be processed to produce and save a mobile-friendly version of the image. The process to produce a mobile-friendly version of the image must start in less than one minute.
You need to design the process that starts the photo processing.
Solution: Move photo processing to an Azure Function triggered from the blob upload.
Does the solution meet the goal?
---
A. Yes *
B. No
---
To view in-depth examples of reacting to Blob storage events by using Azure functions, see these articles:
---
Tutorial: Automate resizing uploaded images using Event Grid;https://docs.microsoft.com/en-us/azure/event-grid/resize-images-on-storage-blob-upload-event?tabs=dotnet
Tutorial: Implement the data lake capture pattern to update a Databricks Delta table;https://docs.microsoft.com/en-us/azure/storage/blobs/data-lake-storage-events
###
You are developing an application that uses Azure Blob storage.
The application must read the transaction logs of all the changes that occur to the blobs and the blob metadata in the storage account for auditing purposes. The changes must be in the order in which they occurred, include only create, update, delete, and copy operations and be retained for compliance reasons.
You need to process the transaction logs asynchronously.
What should you do?
---
A. Process all Azure Blob storage events by using Azure Event Grid with a subscriber Azure Function app.
B. Enable the change feed on the storage account and process all changes for available events. *
C. Process all Azure Storage Analytics logs for successful blob events.
D. Use the Azure Monitor HTTP Data Collector API and scan the request body for successful blob events.
---
Change feed support in Azure Blob Storage
The purpose of the change feed is to provide transaction logs of all the changes that occur to the blobs and the blob metadata in your storage account. The change feed provides ordered, guaranteed, durable, immutable, read-only log of these changes. Client applications can read these logs at any time, either in streaming or in batch mode. The change feed enables you to build efficient and scalable solutions that process change events that occur in your Blob Storage account at a low cost.
The change feed is stored as blobs in a special container in your storage account at standard blob pricing cost. You can control the retention period of these files based on your requirements (See the conditions of the current release). Change events are appended to the change feed as records in the Apache Avro format specification: a compact, fast, binary format that provides rich data structures with inline schema. This format is widely used in the Hadoop ecosystem, Stream Analytics, and Azure Data Factory.
---
Change feed support in Azure Blob Storage;https://docs.microsoft.com/en-us/azure/storage/blobs/storage-blob-change-feed
###
You are developing an application to use Azure Blob storage. You have configured Azure Blob storage to include change feeds.
A copy of your storage account must be created in another region. Data must be copied from the current storage account to the new storage account directly between the storage servers.
You need to create a copy of the storage account in another region and copy the data.
In which order should you perform the actions?

1.Export a template.
2.Modify the template by adding the target region and storage account name.
3.Deploy the template to create the new storage account.
4.Configure the new storage account.
5.Move data to the new storage account.
---
1.2.3.4.5 *
2.3.1.5.4
1.4.3.2.5
2.3.5.1.4
---
To move a storage account, create a copy of your storage account in another region. Then, move your data to that account by using AzCopy, or another tool of your choice.

Export a template.
Modify the template by adding the target region and storage account name.
Deploy the template to create the new storage account.
Configure the new storage account.
Move data to the new storage account.
Delete the resources in the source region.
---
Move an Azure Storage account to another region;https://docs.microsoft.com/en-us/azure/storage/common/storage-account-move?tabs=azure-portal
###
D
You are developing an ASP.NET Core web application. You plan to deploy the application to Azure Web App for Containers.
The application needs to store runtime diagnostic data that must be persisted across application restarts. You have the following code:
az-204-q5.png
You need to configure the application settings so that diagnostic data is stored as required.
---
value true for:;LOCALAPPDATA;WEBSITE_LOCALCACHE_ENABLED;DOTNET_HOSTING_OPTIMIZATION_CACHE;WEBSITE_ENABLE_APP_SERVICE_STORAGE *;
value for DIAGDATA:;/home *;/local;D:\home;D:\local;
---
If WEBSITES_ENABLE_APP_SERVICE_STORAGE setting is unspecified or set to true, the /home/ directory will be shared across scale instances, and files written will persist across restarts. Explicitly setting WEBSITES_ENABLE_APP_SERVICE_STORAGE to false will disable the mount.
---
Custom containers;https://docs.microsoft.com/en-us/azure/app-service/faq-app-service-linux#custom-containers
###
You are developing a web app that is protected by Azure Web Application Firewall (WAF). All traffic to the web app is routed through an Azure Application
Gateway instance that is used by multiple web apps. The web app address is contoso.azurewebsites.net.
All traffic must be secured with SSL. The Azure Application Gateway instance is used by multiple web apps.
You need to configure the Azure Application Gateway for the app.
Which two actions should you perform? Each correct answer presents part of the solution.
NOTE: Each correct selection is worth one point.
---
A. In the Azure Application Gateway's HTTP setting, enable the Use for App service setting. *
B. Convert the web app to run in an Azure App service environment (ASE).
C. Add an authentication certificate for contoso.azurewebsites.net to the Azure Application gateway.
D. In the Azure Application Gateway's HTTP setting, set the value of the Override backend path option to contoso22.azurewebsites.net. *
---
The ability to specify a host override is defined in the HTTP settings and can be applied to any back-end pool during rule creation.
The ability to derive the host name from the IP or FQDN of the back-end pool members. HTTP settings also provide an option to dynamically pick the host name from a back-end pool member's FQDN if configured with the option to derive host name from an individual back-end pool member.
SSL termination and end to end SSL with multi-tenant services.
In case of end to end SSL, trusted Azure services such as Azure App service web apps do not require whitelisting the backends in the application gateway.
Therefore, there is no need to add any authentication certificates.
---
Application Gateway support for multi-tenant back ends such as App service;https://docs.microsoft.com/en-us/azure/application-gateway/application-gateway-web-app-overview
Configure App Service with Application Gateway;https://docs.microsoft.com/en-US/azure/application-gateway/configure-web-app-portal
###
question 7
D
You are implementing a software as a service (SaaS) ASP.NET Core web service that will run as an Azure Web App. The web service will use an on-premises
SQL Server database for storage. The web service also includes a WebJob that processes data updates. Four customers will use the web service.
✑ Each instance of the WebJob processes data for a single customer and must run as a singleton instance.
✑ Each deployment must be tested by using deployment slots prior to serving production data.
✑ Azure costs must be minimized.
✑ Azure resources must be located in an isolated network.
You need to configure the App Service plan for the Web App.
How should you configure the App Service plan?
---
Number of VM instances:;2;4 *;8;16;
Pricing tier:;Isolated *;Standard;Premium;Consumption;
---
---
Announcing App Service Isolated, more power, scale and ease of use;https://azure.microsoft.com/sv-se/blog/announcing-app-service-isolated-more-power-scale-and-ease-of-use/
###
D
You are a developer for a software as a service (SaaS) company that uses an Azure Function to process orders. The Azure Function currently runs on an Azure
Function app that is triggered by an Azure Storage queue.
You are preparing to migrate the Azure Function to Kubernetes using Kubernetes-based Event Driven Autoscaling (KEDA).
You need to configure Kubernetes Custom Resource Definitions (CRD) for the Azure Function.
Which CRDs should you configure?
---
Azure Function code:;Secret;Deployment;ScaledObject;TriggerAuthentication;
Pooling Interval:;Azure Function code:;Secret;Deployment;ScaledObject;TriggerAuthentication;
Azure Storage connection string:;Azure Function code:;Secret;Deployment;ScaledObject;TriggerAuthentication;
---
Deployment - To deploy Azure Functions to Kubernetes use the func kubernetes deploy command has several attributes that directly control how our app scales, once it is deployed to Kubernetes.
ScaledObject - With --polling-interval, we can control the interval used by KEDA to check Azure Service Bus Queue for messages.
Secret - Store connection strings in Kubernetes Secrets.

KEDA is a Kubernetes-based Event Driven Autoscaler. With KEDA, you can drive the scaling of any container in Kubernetes based on the number of events needing to be processed.

KEDA is a single-purpose and lightweight component that can be added into any Kubernetes cluster. KEDA works alongside standard Kubernetes components like the Horizontal Pod Autoscaler and can extend functionality without overwriting or duplication. With KEDA you can explicitly map the apps you want to use event-driven scale, with other apps continuing to function. This makes KEDA a flexible and safe option to run alongside any number of any other Kubernetes applications or frameworks.
---
Serverless Workloads In Kubernetes With KEDA;https://www.thinktecture.com/en/kubernetes/serverless-workloads-with-keda/serverless-workloads-with-keda/
Kubernetes Event-driven Autoscaling;https://keda.sh/
###
D
---
You are creating a CLI script that creates an Azure web app and related services in Azure App Service. The web app uses the following variables:
$gitrepo = https://github.com/contoso/webapp
@webappname = Webapp1103
You need to automatically deploy code from Git-Hub to the newly created web app.
How should you complete the script?
az group create --location westeurope --name myResourceGroup
PLACEHOLDER1 --name $webappname --resource-group myResourceGroup --sku FREE
PLACEHOLDER2 --name $webappname --resource-group myResourceGroup
PLACEHOLDER3 PLACEHOLDER4 source config --name $webappname --resourceGroup PLACEHOLDER5
---
PLACEHOLDER1:;az web app create;az appservice plan create *;az webapp deployment;az group delete;
PLACEHOLDER2:;az web app create *;az appservice plan create;az webapp deployment;az group delete;
PLACEHOLDER3:;--repo-url $gitrepo --branch master --manual-integration;git clone $gitrepo;--plan $webappname *;
PLACEHOLDER4:;az web app create;az appservice plan create;az webapp deployment *;az group delete;
PLACEHOLDER5:;--repo-url $gitrepo --branch master --manual-integration *;git clone $gitrepo;--plan $webappname;
---
az appservice plan create - The azure group creates command successfully returns JSON result. Now we can use resource group to create a azure app service plan
az webapp create - Create a new web app..
--plan $webappname - ..with the serviceplan we created
az webapp deployment - Continuous Delivery with GitHub.
Example: az webapp deployment source config --name firstsamplewebsite1 --resource-group websites--repo-url $gitrepo --branch master --git-token $token
-repo-url $gitrepo --branch master --manual-integration
---
Create an App Service app and deploy code to a staging environment using Azure CLI;https://docs.microsoft.com/en-us/azure/app-service/scripts/cli-deploy-staging-environment
###
You develop a software as a service (SaaS) offering to manage photographs. Users upload photos to a web service which then stores the photos in Azure
Storage Blob storage. The storage account type is General-purpose V2.
When photos are uploaded, they must be processed to produce and save a mobile-friendly version of the image. The process to produce a mobile-friendly version of the image must start in less than one minute.
You need to design the process that starts the photo processing.
Solution: Trigger the photo processing from Blob storage events.
Does the solution meet the goal?
---
A. Yes *
B. No
---
You need to catch the triggered event, so move the photo processing to an Azure Function triggered from the blob upload.
Events are pushed using Azure Event Grid to subscribers such as Azure Functions, Azure Logic Apps, or even to your own http listener.
---
Reacting to Blob storage events;https://docs.microsoft.com/en-us/azure/storage/blobs/storage-blob-event-overview
###
You develop Azure solutions.
You must connect to a No-SQL globally-distributed database by using the .NET API.
You need to create an object to configure and execute requests in the database.
Which code segment should you use?
---
A. new Container(EndpointUri, PrimaryKey);
B. new Database(Endpoint, PrimaryKey);
C. new CosmosClient(EndpointUri, PrimaryKey);
---
// Create a new instance of the Cosmos Client
this.cosmosClient = new CosmosClient(EndpointUri, PrimaryKey);
---
Tutorial: Build a .NET console app to manage data in Azure Cosmos DB SQL API account;https://docs.microsoft.com/en-us/azure/cosmos-db/sql-api-get-started
###
D
You are building a traffic monitoring system that monitors traffic along six highways. The system produces time series analysis-based reports for each highway.
Data from traffic sensors are stored in Azure Event Hub.
Traffic data is consumed by four departments. Each department has an Azure Web App that displays the time series-based reports and contains a WebJob that processes the incoming data from Event Hub. All Web Apps run on App Service Plans with three instances.
Data throughput must be maximized. Latency must be minimized.
You need to implement the Azure Event Hub.
Which settings should you use? 
---
Number of partitions:;3;4;6 *;12;
Partition Key:;Highway *;Department;Timestamp;VM name;
---
The number of partitions is specified at creation and must be between 2 and 32. There are 6 highways.
Event Hubs provides message streaming through a partitioned consumer pattern in which each consumer only reads a specific subset, or partition, of the message stream. This pattern enables horizontal scale for event processing and provides other stream-focused features that are unavailable in queues and topics.
A partition is an ordered sequence of events that is held in an event hub. As newer events arrive, they are added to the end of this sequence. A partition can be thought of as a "commit log."
Event Hubs retains data for a configured retention time that applies across all partitions in the event hub. Events expire on a time basis; you cannot explicitly delete them. Because partitions are independent and contain their own sequence of data, they often grow at different rates.
---
Features and terminology in Azure Event Hubs;https://docs.microsoft.com/en-us/azure/event-hubs/event-hubs-features
###
D
You are developing a microservices solution. You plan to deploy the solution to a multinode Azure Kubernetes Service (AKS) cluster.
You need to deploy a solution that includes the following features:
✑ reverse proxy capabilities
✑ configurable traffic routing
✑ TLS termination with a custom certificate
Which component should you use?
---
DeploySolution;Helm *;Draft;Brigade;KubeCtl;Ingress Controller;CoreDNS;Virtual Kubelet;
View cluster and external IP addressing;Helm;Draft;Brigade;KubeCtl *;Ingress Controller;CoreDNS;Virtual Kubelet;
Implement a single, public IP endpoint that is routed to multiple microservices;Helm;Draft;Brigade;KubeCtl;Ingress Controller *;CoreDNS;Virtual Kubelet;
---
Helm - To create the ingress controller, use Helm to install nginx-ingress.
kubectl - To find the cluster IP address of a Kubernetes pod, use the kubectl get pod command on your local machine, with the option -o wide.
Ingress Controller - An ingress controller is a piece of software that provides reverse proxy, configurable traffic routing, and TLS termination for Kubernetes services. Kubernetes ingress resources are used to configure the ingress rules and routes for individual Kubernetes services.
---
Create an ingress controller in Azure Kubernetes Service (AKS);https://docs.microsoft.com/bs-cyrl-ba/azure/aks/ingress-basic
###
Your company is developing an Azure API.
You need to implement authentication for the Azure API. You have the following requirements:
✑ All API calls must be secure.
✑ Callers to the API must not send credentials to the API.
Which authentication mechanism should you use?
---
A. Basic
B. Anonymous
C. Managed identity *
D. Client certificate
---
Use the authentication-managed-identity policy to authenticate with a backend service using the managed identity of the API Management service. This policy essentially uses the managed identity to obtain an access token from Azure Active Directory for accessing the specified resource. After successfully obtaining the token, the policy will set the value of the token in the Authorization header using the Bearer scheme.
---
API Management authentication policies;https://docs.microsoft.com/bs-cyrl-ba/azure/api-management/api-management-authentication-policies
###
You are a developer for a SaaS company that offers many web services.
All web services for the company must meet the following requirements:
✑ Use API Management to access the services
✑ Use OpenID Connect for authentication

Prevent anonymous usage -

A recent security audit found that several web services can be called without any authentication.
Which API Management policy should you implement?
---
A. jsonp
B. authentication-certificate
C. check-header
D. validate-jwt
---
Add the validate-jwt policy to validate the OAuth token for every incoming request.
The validate-jwt policy enforces existence and validity of a JWT extracted from either a specified HTTP Header or a specified query parameter.
---
Validate JWT;https://docs.microsoft.com/en-us/azure/api-management/api-management-access-restriction-policies#ValidateJWT
###
D
Contoso, Ltd. provides an API to customers by using Azure API Management (APIM). The API authorizes users with a JWT token.
You must implement response caching for the APIM gateway. The caching mechanism must detect the user ID of the client that accesses data for a given location and cache the response for that user ID.
You need to add the following policies to the policies file:
✑ a set-variable policy to store the detected user identity
✑ a cache-lookup-value policy
✑ a cache-store-value policy
a find-and-replace policy to update the response body with the user profile information

To which policy section should you add the policies? 
---
Set-variable;Inbound *;Outbound;
Cache-lookup-value;Inbound *;Outbound;
Cache-store-value;Inbound;Outbound *;
Find-and-replace;Inbound;Outbound *;
---
The set-variable policy declares a context variable and assigns it a value specified via an expression or a string literal. if the expression contains a literal it will be converted to a string and the type of the value will be System.String.
Use the cache-lookup-value policy to perform cache lookup by key and return a cached value. The key can have an arbitrary string value and is typically provided using a policy expression.
The cache-store-value performs cache storage by key. The key can have an arbitrary string value and is typically provided using a policy expression.
The find-and-replace policy finds a request or response substring and replaces it with a different substring.
---
API Management caching policies;https://docs.microsoft.com/en-us/azure/api-management/api-management-caching-policies
###
You develop a web application.
You need to register the application with an active Azure Active Directory (Azure AD) tenant.
Which three actions should you perform in sequence?
1. Select Manifest from the middle-tier service registration
2. In Enterprise Applications, select New application
3. Add a Cryptographic key
4. Create a new application and provide the name, account type and redirect URL
5. Select the Azure AD instance
6. Use an access token to access the secure resource
7. In App Registrations, select New registration
---
7,5,4 *
2,5,4
2,1,3
7,3,1
---
Sign in to the Azure portal.

If you have access to multiple tenants, use the Directory + subscription filter  in the top menu to select the tenant in which you want to register an application.

Search for and select Azure Active Directory.

Under Manage, select App registrations, then New registration.

Enter a Name for your application. Users of your app might see this name, and you can change it later.

Specify who can use the application, sometimes referred to as the sign-in audience.

Enter Redirect URI (optional)

Select Register to complete the initial app registration.
---
Quickstart: Register an application with the Microsoft identity platform;https://docs.microsoft.com/en-us/azure/active-directory/develop/quickstart-register-app
###
C
You have a new Azure subscription. You are developing an internal website for employees to view sensitive data. The website uses Azure Active Directory (Azure AD) for authentication.
You need to implement multifactor authentication for the website.
Which two actions should you perform? Each correct answer presents part of the solution.
NOTE: Each correct selection is worth one point.
---
A. Configure the website to use Azure AD B2C.
B. In Azure AD, create a new conditional access policy. *
C. Upgrade to Azure AD Premium. *
D. In Azure AD, enable application proxy.
E. In Azure AD conditional access, enable the baseline policy.
---
MFA Enabled by conditional access policy. It is the most flexible means to enable two-step verification for your users. Enabling using conditional access policy only works for Azure MFA in the cloud and is a premium feature of Azure AD.
Multi-Factor Authentication comes as part of the following offerings:
Azure Active Directory Premium licenses - Full featured use of Azure Multi-Factor Authentication Service (Cloud) or Azure Multi-Factor Authentication Server (On-premises).
Multi-Factor Authentication for Office 365
Azure Active Directory Global Administrators
---
Plan an Azure Multi-Factor Authentication deployment;https://docs.microsoft.com/en-us/azure/active-directory/authentication/howto-mfa-getstarted
###
C
You are developing an ASP.NET Core Web API web service. The web service uses Azure Application Insights for all telemetry and dependency tracking. The web service reads and writes data to a database other than Microsoft SQL Server.
You need to ensure that dependency tracking works for calls to the third-party database.
Which two dependency telemetry properties should you use? Each correct answer presents part of the solution.
NOTE: Each correct selection is worth one point.
---
A. Telemetry.Context.Cloud.RoleInstance
B. Telemetry.Id *
C. Telemetry.Name
D. Telemetry.Context.Operation.Id *
E. Telemetry.Context.Session.Id
---
---
Track custom operations with Application Insights .NET SDK;https://docs.microsoft.com/en-us/azure/azure-monitor/app/custom-operations-tracking#enqueue
###
C
You are using Azure Front Door Service.
You are expecting inbound files to be compressed by using Brotli compression. You discover that inbound XML files are not compressed. The files are 9 megabytes (MB) in size.
You need to determine the root cause for the issue.
To answer, select the appropriate options in the answer area.
Choose all that apply:
---
The file MIME type is supported by the service *
Edge nodes must be purged of all cache assets
The compression type is supported *
---
Front Door can dynamically compress content on the edge, resulting in a smaller and faster response to your clients. All files are eligible for compression. However, a file must be of a MIME type that is eligible for compression list.
Sometimes you may wish to purge cached content from all edge nodes and force them all to retrieve new updated assets. This might be due to updates to your web application, or to quickly update assets that contain incorrect information.
These profiles support the following compression encodings: Gzip (GNU zip), Brotli
---
Caching with Azure Front Door;https://docs.microsoft.com/en-us/azure/frontdoor/front-door-caching
###
D
You are developing an Azure App Service hosted ASP.NET Core web app to deliver video on-demand streaming media. You enable an Azure Content Delivery
Network (CDN) Standard for the web endpoint. Customer videos are downloaded from the web app by using the following example URL.: http://www.contoso.com/ content.mp4?quality=1
All media content must expire from the cache after one hour. Customer videos with varying quality must be delivered to the closest regional point of presence (POP) node.
You need to configure Azure CDN caching rules.
Which options should you use?
---
Caching behavior:;Bypass cache;Override *;Set if missing;
Cache expiration duration:;1 second;1 minute;1 hour *;1 day;
Query string caching behavior:;Ignore query strings;Bypass caching for query strings;Cache every unique URL *;
---
Override: Ignore origin-provided cache duration; use the provided cache duration instead. This will not override cache-control: no-cache. Set if missing: Honor origin-provided cache-directive headers, if they exist; otherwise, use the provided cache duration.
All media content must expire from the cache after one hour.
Cache every unique URL: In this mode, each request with a unique URL, including the query string, is treated as a unique asset with its own cache. For example, the response from the origin server for a request for example.ashx?q=test1 is cached at the POP node and returned for subsequent caches with the same query string. A request for example.ashx?q=test2 is cached as a separate asset with its own time-to-live setting.
---
Control Azure CDN caching behavior with caching rules;https://docs.microsoft.com/en-us/azure/cdn/cdn-caching-rules
###
C
You develop a web app that uses tier D1 app service plan by using the Web Apps feature of Microsoft Azure App Service.
Spikes in traffic have caused increases in page load times.
You need to ensure that the web app automatically scales when CPU load is about 85 percent and minimize costs.
Which four actions should you perform in sequence?
1.Configure the web app to the Premium App Service tier.
2. Configure the web app to the Standard App Service tier.
3. Enable autoscaling on the web-app.
4. Add a Scale rule.
5. Switch to an Azure App Services consumption plan.
6. Configure a Scale condition
---
2,3,6,4 *
1,3,6,4
5,3,4,6
2,3,3,6
---
The Standard tier supports auto-scaling, and we should minimize the cost.
---
Get started with Autoscale in Azure;https://docs.microsoft.com/en-us/azure/azure-monitor/platform/autoscale-get-started
###
You are developing an Azure Service application that processes queue data when it receives a message from a mobile application. Messages may not be sent to the service consistently.
You have the following requirements:
✑ Queue size must not grow larger than 80 gigabytes (GB).
✑ Use first-in-first-out (FIFO) ordering of messages.
✑ Minimize Azure costs.
You need to implement the messaging solution.
Solution: Use the .Net API to add a message to an Azure Service Bus Queue from the mobile application. Create an Azure Function App that uses an Azure
Service Bus Queue trigger.
Does the solution meet the goal?
---
Yes *
No
---
---
Storage queues and Service Bus queues - compared and contrasted;https://docs.microsoft.com/en-us/azure/service-bus-messaging/service-bus-azure-and-service-bus-queues-compared-contrasted
###
You are developing an Azure solution to collect point-of-sale (POS) device data from 2,000 stores located throughout the world. A single device can produce 2 megabytes (MB) of data every 24 hours. Each store location has one to five devices that send data.
You must store the device data in Azure Blob storage. Device data must be correlated based on a device identifier. Additional stores are expected to open in the future.
You need to implement a solution to receive the device data.
Solution: Provision an Azure Notification Hub. Register all devices with the hub.
Does the solution meet the goal?
---
Yes
No *
---
Service Bus is intended for traditional enterprise applications. These enterprise applications require transactions, ordering, duplicate detection, and instantaneous consistency. Service Bus enables cloud-native applications to provide reliable state transition management for business processes. When handling high-value messages that cannot be lost or duplicated, use Azure Service Bus. Service Bus also facilitates highly secure communication across hybrid cloud solutions and can connect existing on-premises systems to cloud solutions.

Service Bus is a brokered messaging system. It stores messages in a "broker" (for example, a queue) until the consuming party is ready to receive the messages.

It has the following characteristics:

reliable asynchronous message delivery (enterprise messaging as a service) that requires polling
advanced messaging features like FIFO, batching/sessions, transactions, dead-lettering, temporal control, routing and filtering, and duplicate detection
at least once delivery
optional in-order delivery
---
Choose between Azure messaging services - Event Grid, Event Hubs, and Service Bus;https://docs.microsoft.com/en-us/azure/event-grid/compare-messaging-services
###
You are developing an Azure solution to collect point-of-sale (POS) device data from 2,000 stores located throughout the world. A single device can produce 2 megabytes (MB) of data every 24 hours. Each store location has one to five devices that send data.
You must store the device data in Azure Blob storage. Device data must be correlated based on a device identifier. Additional stores are expected to open in the future.
You need to implement a solution to receive the device data.
Solution: Provision an Azure Service Bus. Configure a topic to receive the device data by using a correlation filter.
Does the solution meet the goal?
---
Yes *
No
---
Service Bus is intended for traditional enterprise applications. These enterprise applications require transactions, ordering, duplicate detection, and instantaneous consistency. Service Bus enables cloud-native applications to provide reliable state transition management for business processes. When handling high-value messages that cannot be lost or duplicated, use Azure Service Bus. Service Bus also facilitates highly secure communication across hybrid cloud solutions and can connect existing on-premises systems to cloud solutions.

Service Bus is a brokered messaging system. It stores messages in a "broker" (for example, a queue) until the consuming party is ready to receive the messages.

It has the following characteristics:

reliable asynchronous message delivery (enterprise messaging as a service) that requires polling
advanced messaging features like FIFO, batching/sessions, transactions, dead-lettering, temporal control, routing and filtering, and duplicate detection
at least once delivery
optional in-order delivery
---
Choose between Azure messaging services - Event Grid, Event Hubs, and Service Bus;https://docs.microsoft.com/en-us/azure/event-grid/compare-messaging-services
###
You are developing an Azure solution to collect point-of-sale (POS) device data from 2,000 stores located throughout the world. A single device can produce 2 megabytes (MB) of data every 24 hours. Each store location has one to five devices that send data.
You must store the device data in Azure Blob storage. Device data must be correlated based on a device identifier. Additional stores are expected to open in the future.
You need to implement a solution to receive the device data.
Solution: Provision an Azure Event Grid. Configure event filtering to evaluate the device identifier.
Does the solution meet the goal?
---
Yes
No *
---
Instead use an Azure Service Bus, which is used order processing and financial transactions.
Note: An event is a lightweight notification of a condition or a state change. Event hubs is usually used reacting to status changes.
---
Choose between Azure messaging services - Event Grid, Event Hubs, and Service Bus;https://docs.microsoft.com/en-us/azure/event-grid/compare-messaging-services
###
D
You manage several existing Logic Apps.
You need to change definitions, add new logic, and optimize these apps on a regular basis.
What should you use? 
---
Edit B2B workflows:;Logic Apps Designer;Code View Editor;Enterprise Intergration Pack *;
Edit definitions in JSON:;Logic Apps Designer;Code View Editor *;Enterprise Intergration Pack;
Visually and functionality:;Logic Apps Designer *;Code View Editor;Enterprise Intergration Pack;
---
For business-to-business (B2B) solutions and seamless communication between organizations, you can build automated scalable enterprise integration workflows by using the Enterprise Integration Pack (EIP) with Azure Logic Apps.
Edit JSON - Azure portal -
1. Sign in to the Azure portal.
2. From the left menu, choose All services. In the search box, find "logic apps", and then from the results, select your logic app.
3. On your logic app's menu, under Development Tools, select Logic App Code View.
4. The Code View editor opens and shows your logic app definition in JSON format.
---
B2B enterprise integration solutions with Azure Logic Apps and Enterprise Integration Pack;https://docs.microsoft.com/en-us/azure/logic-apps/logic-apps-enterprise-integration-overview
Create, edit, or extend JSON for logic app workflow definitions in Azure Logic Apps;https://docs.microsoft.com/en-us/azure/logic-apps/logic-apps-author-definitions
###
A company is developing a solution that allows smart refrigerators to send temperature information to a central location. You have an existing Service Bus.
The solution must receive and store message until they can be processed. You create an Azure Service Bus Instance by providing a name, pricing tier, subscription, resource group, and location.
You need to complete the configuration.
Which Azure CLI or PowerShell command should you run?
---
az servicebus queue create --resource-group fridge-group --namespace-name fridge-ns --name fridge-q *
New-AzureRmResourceGroup -Name fridge-rg -Location fridge-loc
New-AzureRmServiceBusNamespace -ResourceGroupName fridge-loc -Location fridge-loc
connectionString=$(az servicebus namespace authorization-rule keys list --resource-group $resourceGroupName --namespace-name $namespaceName --name RootManageSharedAccessKey --query primaryConnectionString --output tsv)
---
Run the following command to create a queue in the namespace: az servicebus queue create --resource-group ContosoRG --namespace-name ContosoSBusNS --name ContosoOrdersQueue
---
C
You are working for Contoso, Ltd.
You define an API Policy object by using the following XML markup:
Use the Azure CLI to create a Service Bus namespace and a queue;https://docs.microsoft.com/en-us/azure/service-bus-messaging/service-bus-quickstart-cli
###
<set-variable name="bodySize" value="@(context.Request.Headers["Content-Length"][0])" />
<choose>
    <when condition="@(int.Parse(context.Variables.GetValueOrDefault<string>("bodySize")) < 512 000)">
    </when>
    <otherwise>
        <rewrite-uri template = "/put" />
        <set-backend-service base-url="http://contoso.com/api/9.1/" />
    </otherwise>
</choose>
Choose all that apply:
---
The XML segment belongs in the <inbound> section of the policy. *
If the body size is >256k, an error will occur.
If the request is http://contoso/com/api/9.2/, the policy will retain the higher version
---
The choose policy applies enclosed policy statements based on the outcome of evaluation of Boolean expressions, similar to an if-then-else or a switch construct in a programming language.
<choose>
    <when condition="Boolean expression | Boolean constant">
        <!— one or more policy statements to be applied if the above condition is true  -->
    </when>
    <when condition="Boolean expression | Boolean constant">
        <!— one or more policy statements to be applied if the above condition is true  -->
    </when>
    <otherwise>
        <!— one or more policy statements to be applied if none of the above conditions are true  -->
</otherwise>
</choose>
---
API Management advanced policies:https://docs.microsoft.com/en-us/azure/api-management/api-management-advanced-policies#choose
###
C
You are developing a solution that will use Azure messaging services.
You need to ensure that the solution uses a publish-subscribe model and eliminates the need for constant polling.
What are three possible ways to achieve the goal? Each correct answer presents a complete solution.
---
A. Service Bus *
B. Event Hub *
C. Event Grid *
D. Queue
---
Event Hubs is a fully managed, real-time data ingestion service that’s simple, trusted, and scalable.
It is strongly recommended to use available messaging products and services that support a publish-subscribe model, rather than building your own. In Azure, consider using Service Bus or Event Grid. Other technologies that can be used for pub/sub messaging include Redis, RabbitMQ, and Apache Kafka.
---
Publisher-Subscriber pattern;https://docs.microsoft.com/en-us/azure/architecture/patterns/publisher-subscriber
###
A company is implementing a publish-subscribe (Pub/Sub) messaging component by using Azure Service Bus. You are developing the first subscription application.
In the Azure portal you see that messages are being sent to the subscription for each topic. You create and initialize a subscription client object by supplying the correct details, but the subscription application is still not consuming the messages.
You need to ensure that the subscription client processes all messages.
Which code segment should you use?
---
A. await subscriptionClient.AddRuleAsync(new RuleDescription(RuleDescription.DefaultRuleName, new TrueFilter()));
B. subscriptionClient = new SubscriptionClient(ServiceBusConnectionString, TopicName, SubscriptionName);
C. await subscriptionClient.CloseAsync();
D. subscriptionClient.RegisterMessageHandler(ProcessMessageAsync, messageHandlerOptions); *
---
Using topic client, call RegisterMessageHandler which is used to receive messages continuously from the entity. It registers a message handler and begins a new thread to receive messages. This handler is waited on every time a new message is received by the receiver. subscriptionClient.RegisterMessageHandler(ReceiveMessagesAsync, messageHandlerOptions);
---
Azure Service Bus Topic And Subscription (Pub-Sub);https://www.c-sharpcorner.com/article/azure-service-bus-topic-and-subscription-pub-sub/
###
You are developing an Azure Service application that processes queue data when it receives a message from a mobile application. Messages may not be sent to the service consistently.
You have the following requirements:
✑ Queue size must not grow larger than 80 gigabytes (GB).
✑ Use first-in-first-out (FIFO) ordering of messages.
✑ Minimize Azure costs.
You need to implement the messaging solution.
Solution: Use the .Net API to add a message to an Azure Storage Queue from the mobile application. Create an Azure VM that is triggered from Azure Storage
Queue events.
Does the solution meet the goal?
---
A. Yes
B. No *
---
Don't use a VM, instead create an Azure Function App that uses an Azure Service Bus Queue trigger.
---
Create a function triggered by Azure Queue storage;https://docs.microsoft.com/en-us/azure/azure-functions/functions-create-storage-queue-triggered-function
###
You are developing an Azure Service application that processes queue data when it receives a message from a mobile application. Messages may not be sent to the service consistently.
You have the following requirements:
✑ Queue size must not grow larger than 80 gigabytes (GB).
✑ Use first-in-first-out (FIFO) ordering of messages.
✑ Minimize Azure costs.
You need to implement the messaging solution.
Solution: Use the .Net API to add a message to an Azure Service Bus Queue from the mobile application. Create an Azure Windows VM that is triggered from
Azure Service Bus Queue.
Does the solution meet the goal?
---
A. Yes
B. No *
---
Don't use a VM, instead create an Azure Function App that uses an Azure Service Bus Queue trigger.
---
Create a function triggered by Azure Queue storage;https://docs.microsoft.com/en-us/azure/azure-functions/functions-create-storage-queue-triggered-function
###
C
ContentAnalysisService -
The company's data science group built ContentAnalysisService which accepts user generated content as a string and returns a probable value for inappropriate content. Any values over a specific threshold must be reviewed by an employee of Contoso, Ltd.
You must create an Azure Function named CheckUserContent to perform the content checks.

Costs -
You must minimize costs for all Azure services.

Manual review -
To review content, the user must authenticate to the website portion of the ContentAnalysisService using their Azure AD credentials. The website is built using
React and all pages and API endpoints require authentication. In order to review content a user must be part of a ContentReviewer role. All completed reviews must include the reviewer's email address for auditing purposes.

High availability -
All services must run in multiple regions. The failure of any service in a region must not impact overall application availability.

Monitoring -
An alert must be raised if the ContentUploadService uses more than 80 percent of available CPU-cores.

Security -
You have the following security requirements:
Any web service accessible over the Internet must be protected from cross site scripting attacks.
All websites and services must use SSL from a valid root certificate authority.
Azure Storage access keys must only be stored in memory and must be available only to the service.
All Internal services must only be accessible from Internal Virtual Networks (VNets)
All parts of the system must support inbound and outbound traffic restrictions.
All service calls must be authenticated by using Azure AD.

User agreements -
When a user submits content, they must agree to a user agreement. The agreement allows employees of Contoso.Ltd to review content, store cookies on user devices and track user's IP addresses.
Information regarding agreements is used by multiple divisions within Contoso, Ltd.
User responses must not be lost and must be available to all parties regardless of individual service uptime. The volume of agreements is expected to be in the millions per hour.

Validation testing -
When a new version of the ContentAnalysisService is available the previous seven days of content must be processed with the new version to verify that the new version does not significantly deviate from the old version.

Issues -
Users of the ContentUploadService report that they occasionally see HTTP 502 responses on specific pages.

Code -
az-204-q40.png
az-204-q40-1.png
Question
You need to configure the ContentUploadService deployment.
Which two actions should you perform? Each correct answer presents part of the solution.
NOTE: Each correct selection is worth one point.
---
A. Add the following markup to line CS23: types: Private *
B. Add the following markup to line CS24: osType: Windows
C. Add the following markup to line CS24: osType: Linux *
D. Add the following markup to line CS23: types: Public
---
Since all of the services must be available from a virtual network, we have to mention the types as Private
Since all of the services must be available from a virtual network, we have to mention the osType as Linux. Currently this is the only OS that supports container instances to be available from a virtual network.
---
Virtual network scenarios and resources;https://docs.microsoft.com/en-us/azure/container-instances/container-instances-virtual-network-concepts#unsupported-networking-scenarios
###
D
Current environment -
Windows Server 2016 virtual machine
The virtual machine (VM) runs BizTalk Server 2016. The VM runs the following workflows:
Ocean Transport "" This workflow gathers and validates container information including container contents and arrival notices at various shipping ports.
Inland Transport "" This workflow gathers and validates trucking information including fuel usage, number of stops, and routes.
The VM supports the following REST API calls:
Container API "" This API provides container information including weight, contents, and other attributes.
Location API "" This API provides location information regarding shipping ports of call and tracking stops.
Shipping REST API "" This API provides shipping information for use and display on the shipping website.

Shipping Data -
The application uses MongoDB JSON document storage database for all container and transport information.

Shipping Web Site -
The site displays shipping container tracking information and container contents. The site is located at http://shipping.wideworldimporters.com/

Proposed solution -
The on-premises shipping application must be moved to Azure. The VM has been migrated to a new Standard_D16s_v3 Azure VM by using Azure Site Recovery and must remain running in Azure to complete the BizTalk component migrations. You create a Standard_D16s_v3 Azure VM to host BizTalk Server. The Azure architecture diagram for the proposed solution is shown below:
az-204-q41.png
Requirements -

Shipping Logic app -
The Shipping Logic app must meet the following requirements:
Support the ocean transport and inland transport workflows by using a Logic App.
Support industry-standard protocol X12 message format for various messages including vessel content details and arrival notices.
Secure resources to the corporate VNet and use dedicated storage resources with a fixed costing model.
Maintain on-premises connectivity to support legacy applications and final BizTalk migrations.

Shipping Function app -
Implement secure function endpoints by using app-level security and include Azure Active Directory (Azure AD).

REST APIs -
The REST API's that support the solution must meet the following requirements:
Secure resources to the corporate VNet.
Allow deployment to a testing location within Azure while not incurring additional costs.
Automatically scale to double capacity during peak shipping times while not causing application downtime.
Minimize costs when selecting an Azure payment model.

Shipping data -
Data migration from on-premises to Azure must minimize costs and downtime.

Shipping website -
Use Azure Content Delivery Network (CDN) and ensure maximum performance for dynamic content while minimizing latency and costs.

Issues -

Windows Server 2016 VM -
The VM shows high network latency, jitter, and high CPU utilization. The VM is critical and has not been backed up in the past. The VM must enable a quick restore from a 7-day snapshot to include in-place restore of disks in case of failure.

Shipping website and REST APIs -
The following error message displays while you are testing the website:
Failed to load http://test-shippingapi.wideworldimporters.com/: No 'Access-Control-Allow-Origin' header is present on the requested resource. Origin 'http://test.wideworldimporters.com/' is therefore not allowed access.
You need to configure Azure CDN for the Shipping web site.
Which configuration options should you use?
---
Tier:;Standard *; Premium;
Profile:;Akamai *; Microsoft;
Optimization:;General Web Delivery;Large File Download;Dynamic Site Acceleration *;Video-On-Demand Media Streaming;
---
Dynamic site acceleration (DSA) is available for Azure CDN Standard from Akamai, Azure CDN Standard from Verizon, and Azure CDN Premium from Verizon profiles.
DSA includes various techniques that benefit the latency and performance of dynamic content. Techniques include route and network optimization, TCP optimization, and more.
You can use this optimization to accelerate a web app that includes numerous responses that aren't cacheable. Examples are search results, checkout transactions, or real-time data. You can continue to use core Azure CDN caching capabilities for static data
---
Optimize Azure CDN for the type of content delivery;https://docs.microsoft.com/en-us/azure/cdn/cdn-optimization-overview
###
Requirements -

ContentAnalysisService -
The company's data science group built ContentAnalysisService which accepts user generated content as a string and returns a probable value for inappropriate content. Any values over a specific threshold must be reviewed by an employee of Contoso, Ltd.
You must create an Azure Function named CheckUserContent to perform the content checks.

Costs -
You must minimize costs for all Azure services.

Manual review -
To review content, the user must authenticate to the website portion of the ContentAnalysisService using their Azure AD credentials. The website is built using
React and all pages and API endpoints require authentication. In order to review content a user must be part of a ContentReviewer role. All completed reviews must include the reviewer's email address for auditing purposes.

High availability -
All services must run in multiple regions. The failure of any service in a region must not impact overall application availability.

Monitoring -
An alert must be raised if the ContentUploadService uses more than 80 percent of available CPU-cores.

Security -
You have the following security requirements:
Any web service accessible over the Internet must be protected from cross site scripting attacks.
All websites and services must use SSL from a valid root certificate authority.
Azure Storage access keys must only be stored in memory and must be available only to the service.
All Internal services must only be accessible from Internal Virtual Networks (VNets)
All parts of the system must support inbound and outbound traffic restrictions.
All service calls must be authenticated by using Azure AD.

User agreements -
When a user submits content, they must agree to a user agreement. The agreement allows employees of Contoso.Ltd to review content, store cookies on user devices and track user's IP addresses.
Information regarding agreements is used by multiple divisions within Contoso, Ltd.
User responses must not be lost and must be available to all parties regardless of individual service uptime. The volume of agreements is expected to be in the millions per hour.
Question
You need to store the user agreements.
Where should you store the agreement after it is completed?
---
A. Azure Storage queue
B. Azure Event Hub *
C. Azure Service Bus topic
D. Azure Event Grid topic
---
Azure Event Hub is used for telemetry and distributed data streaming. This service provides a single solution that enables rapid data retrieval for real-time processing as well as repeated replay of stored raw data. It can capture the streaming data into a file for processing and analysis.
It has the following characteristics: low latency, capable of receiving and processing millions of events per second at least once delivery
---
Choose between Azure messaging services - Event Grid, Event Hubs, and Service Bus;https://docs.microsoft.com/en-us/azure/event-grid/compare-messaging-services
###
You develop and deploy an Azure App Service API app to a Windows-hosted deployment slot named Development. You create additional deployment slots named Testing and Production. You enable auto swap on the Production deployment slot.
You need to ensure that scripts run and resources are available before a swap operation occurs.
Solution: Enable auto swap for the Testing slot. Deploy the app to the Testing slot.
Does the solution meet the goal?
---
A. No *
B. Yes
---
Instead update the web.config file to include the applicationInitialization configuration element. Specify custom initialization actions to run the scripts.
Note: Some apps might require custom warm-up actions before the swap. The applicationInitialization configuration element in web.config lets you specify custom initialization actions. The swap operation waits for this custom warm-up to finish before swapping with the target slot. Here's a sample web.config fragment.
<system.webServer>
<applicationInitialization>
<add initializationPage="/" hostName="[app hostname]" />
<add initializationPage="/Home/About" hostName="[app hostname]" />
</applicationInitialization>
</system.webServer>
---
Troubleshoot swaps;https://docs.microsoft.com/en-us/azure/app-service/deploy-staging-slots#troubleshoot-swaps
###
You develop and deploy an Azure App Service API app to a Windows-hosted deployment slot named Development. You create additional deployment slots named Testing and Production. You enable auto swap on the Production deployment slot.
You need to ensure that scripts run and resources are available before a swap operation occurs.
Solution: Disable auto swap. Update the app with a method named statuscheck to run the scripts. Re-enable auto swap and deploy the app to the Production slot.
Does the solution meet the goal?
---
A. No *
B. Yes
---
nstead update the web.config file to include the applicationInitialization configuration element. Specify custom initialization actions to run the scripts.
Note: Some apps might require custom warm-up actions before the swap. The applicationInitialization configuration element in web.config lets you specify custom initialization actions. The swap operation waits for this custom warm-up to finish before swapping with the target slot. Here's a sample web.config fragment.
<system.webServer>
<applicationInitialization>
<add initializationPage="/" hostName="[app hostname]" />
<add initializationPage="/Home/About" hostName="[app hostname]" />
</applicationInitialization>
</system.webServer>
---
Troubleshoot swaps;https://docs.microsoft.com/en-us/azure/app-service/deploy-staging-slots#troubleshoot-swaps
###
---
---
---
###
---
---
---
###
---
---
---
###
---
---
---
###
---
---
---
###
---
---
---
###
---
---
---
###
---
---
---
###
---
---
---
###
---
---
---
###
---
---
---
###
---
---
---
###
---
---
---
###
---
---
---
###
---
---
---
###
---
---
---
###
---
---
---
###
---
---
---
###
---
---
---
###
---
---
---
###
---
---
---
###
---
---
---
###
---
---
---
###
---
---
---
###
---
---
---
###
---
---
---
###
---
---
---
###
---
---
---
###
---
---
---
###
---
---
---
###
---
---
---
###
---
---
---
###
---
---
---
###
---
---
---
###
---
---
---
###
---
---
---
###
---
---
---
###
---
---
---
###
---
---
---
###
---
---
---
###
---
---
---
###
---
---
---
###
---
---
---
###
---
---
---
